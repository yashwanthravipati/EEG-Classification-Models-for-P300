{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ce5b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PF32_2016_12_05_train32FFS001R01.dat', 'PF32_2016_12_05_train32FFS001R02.dat', 'PF32_2016_12_05_train32FFS001R03.dat', 'pf_training.prm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n",
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of labled P300 events:200\n",
      "no of non_attended P300 events:1000\n",
      "no of labled P300 events:200\n",
      "no of non_attended P300 events:1000\n",
      "no of labled P300 events:200\n",
      "no of non_attended P300 events:1000\n",
      "(3600, 32, 192)\n",
      "(3600,)\n",
      "(3600, 4096)\n"
     ]
    }
   ],
   "source": [
    "from DataLoader import load_data\n",
    "\n",
    "subject_name = 'subject_023'\n",
    "src_dir = 'Desktop/eeg-data/Subjects-Data/'\n",
    "##EEG Inception Model Definition. \n",
    "Train_data, Train_data_inception, Train_data_flattened, Train_labels = load_data(src_dir, subject_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82b5f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160, 32, 192, 1)\n",
      "Train on 2160 samples, validate on 720 samples\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.77157, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 7s - loss: 1.7052 - accuracy: 0.8074 - val_loss: 1.7716 - val_accuracy: 0.8333\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.77157 to 1.17145, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.9741 - accuracy: 0.8333 - val_loss: 1.1714 - val_accuracy: 0.8333\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17145 to 0.93108, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 6s - loss: 0.8124 - accuracy: 0.8333 - val_loss: 0.9311 - val_accuracy: 0.8333\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93108 to 0.81835, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.7528 - accuracy: 0.8333 - val_loss: 0.8183 - val_accuracy: 0.8333\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81835 to 0.76962, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.7303 - accuracy: 0.8333 - val_loss: 0.7696 - val_accuracy: 0.8333\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.76962 to 0.74195, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.7046 - accuracy: 0.8333 - val_loss: 0.7419 - val_accuracy: 0.8333\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.74195 to 0.73006, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.6845 - accuracy: 0.8333 - val_loss: 0.7301 - val_accuracy: 0.8333\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73006 to 0.70820, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.6726 - accuracy: 0.8333 - val_loss: 0.7082 - val_accuracy: 0.8333\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.70820 to 0.69446, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.6474 - accuracy: 0.8333 - val_loss: 0.6945 - val_accuracy: 0.8333\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.69446 to 0.68287, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 6s - loss: 0.6398 - accuracy: 0.8333 - val_loss: 0.6829 - val_accuracy: 0.8333\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.68287 to 0.66346, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.6241 - accuracy: 0.8333 - val_loss: 0.6635 - val_accuracy: 0.8333\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.66346 to 0.65368, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.6105 - accuracy: 0.8333 - val_loss: 0.6537 - val_accuracy: 0.8333\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.65368\n",
      "2160/2160 - 5s - loss: 0.6113 - accuracy: 0.8343 - val_loss: 0.6553 - val_accuracy: 0.8333\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.65368 to 0.64016, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.5901 - accuracy: 0.8356 - val_loss: 0.6402 - val_accuracy: 0.8333\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.64016 to 0.63349, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.5777 - accuracy: 0.8347 - val_loss: 0.6335 - val_accuracy: 0.8333\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.63349 to 0.63139, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.5550 - accuracy: 0.8352 - val_loss: 0.6314 - val_accuracy: 0.8347\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.63139\n",
      "2160/2160 - 5s - loss: 0.5653 - accuracy: 0.8352 - val_loss: 0.6386 - val_accuracy: 0.8333\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.63139\n",
      "2160/2160 - 5s - loss: 0.5474 - accuracy: 0.8352 - val_loss: 0.6378 - val_accuracy: 0.8333\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.63139 to 0.62779, saving model to /tmp/checkpoint.h5\n",
      "2160/2160 - 5s - loss: 0.5304 - accuracy: 0.8394 - val_loss: 0.6278 - val_accuracy: 0.8333\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.5227 - accuracy: 0.8444 - val_loss: 0.6342 - val_accuracy: 0.8347\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.5101 - accuracy: 0.8431 - val_loss: 0.6539 - val_accuracy: 0.8333\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4904 - accuracy: 0.8458 - val_loss: 0.6714 - val_accuracy: 0.8347\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4982 - accuracy: 0.8481 - val_loss: 0.6738 - val_accuracy: 0.8333\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4871 - accuracy: 0.8481 - val_loss: 0.6802 - val_accuracy: 0.8333\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4730 - accuracy: 0.8505 - val_loss: 0.6690 - val_accuracy: 0.8347\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4790 - accuracy: 0.8574 - val_loss: 0.6899 - val_accuracy: 0.8347\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4740 - accuracy: 0.8588 - val_loss: 0.7103 - val_accuracy: 0.8347\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4716 - accuracy: 0.8542 - val_loss: 0.7205 - val_accuracy: 0.8319\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4624 - accuracy: 0.8616 - val_loss: 0.6706 - val_accuracy: 0.8361\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4620 - accuracy: 0.8602 - val_loss: 0.6722 - val_accuracy: 0.8347\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4512 - accuracy: 0.8644 - val_loss: 0.7272 - val_accuracy: 0.8347\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4498 - accuracy: 0.8579 - val_loss: 0.6871 - val_accuracy: 0.8347\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4531 - accuracy: 0.8657 - val_loss: 0.7244 - val_accuracy: 0.8333\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4329 - accuracy: 0.8727 - val_loss: 0.7420 - val_accuracy: 0.8347\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4562 - accuracy: 0.8630 - val_loss: 0.6502 - val_accuracy: 0.8347\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4255 - accuracy: 0.8713 - val_loss: 0.6850 - val_accuracy: 0.8403\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4433 - accuracy: 0.8667 - val_loss: 0.6932 - val_accuracy: 0.8361\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4218 - accuracy: 0.8736 - val_loss: 0.7016 - val_accuracy: 0.8361\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4176 - accuracy: 0.8727 - val_loss: 0.7157 - val_accuracy: 0.8361\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4304 - accuracy: 0.8731 - val_loss: 0.7545 - val_accuracy: 0.8319\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4295 - accuracy: 0.8718 - val_loss: 0.7258 - val_accuracy: 0.8319\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4350 - accuracy: 0.8713 - val_loss: 0.7159 - val_accuracy: 0.8375\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4207 - accuracy: 0.8741 - val_loss: 0.6600 - val_accuracy: 0.8347\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4201 - accuracy: 0.8722 - val_loss: 0.6963 - val_accuracy: 0.8361\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4111 - accuracy: 0.8838 - val_loss: 0.7152 - val_accuracy: 0.8347\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4073 - accuracy: 0.8778 - val_loss: 0.7196 - val_accuracy: 0.8375\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4150 - accuracy: 0.8759 - val_loss: 0.6614 - val_accuracy: 0.8389\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4224 - accuracy: 0.8773 - val_loss: 0.7290 - val_accuracy: 0.8361\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4016 - accuracy: 0.8810 - val_loss: 0.7262 - val_accuracy: 0.8333\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4139 - accuracy: 0.8731 - val_loss: 0.7470 - val_accuracy: 0.8361\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4206 - accuracy: 0.8769 - val_loss: 0.8114 - val_accuracy: 0.8319\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4156 - accuracy: 0.8731 - val_loss: 0.7094 - val_accuracy: 0.8361\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3987 - accuracy: 0.8815 - val_loss: 0.6803 - val_accuracy: 0.8375\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3979 - accuracy: 0.8833 - val_loss: 0.7152 - val_accuracy: 0.8361\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3899 - accuracy: 0.8847 - val_loss: 0.7545 - val_accuracy: 0.8361\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3931 - accuracy: 0.8829 - val_loss: 0.7175 - val_accuracy: 0.8403\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3908 - accuracy: 0.8894 - val_loss: 0.7547 - val_accuracy: 0.8389\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3952 - accuracy: 0.8903 - val_loss: 0.7457 - val_accuracy: 0.8375\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4096 - accuracy: 0.8838 - val_loss: 0.7445 - val_accuracy: 0.8361\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3751 - accuracy: 0.8926 - val_loss: 0.7832 - val_accuracy: 0.8361\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3957 - accuracy: 0.8843 - val_loss: 0.7067 - val_accuracy: 0.8375\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3776 - accuracy: 0.8935 - val_loss: 0.7047 - val_accuracy: 0.8431\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4127 - accuracy: 0.8856 - val_loss: 0.7031 - val_accuracy: 0.8389\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3795 - accuracy: 0.8894 - val_loss: 0.7751 - val_accuracy: 0.8389\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3950 - accuracy: 0.8870 - val_loss: 0.7831 - val_accuracy: 0.8333\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3864 - accuracy: 0.8894 - val_loss: 0.7887 - val_accuracy: 0.8333\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3726 - accuracy: 0.8903 - val_loss: 0.6878 - val_accuracy: 0.8431\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3866 - accuracy: 0.8875 - val_loss: 0.7034 - val_accuracy: 0.8403\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3986 - accuracy: 0.8852 - val_loss: 0.7767 - val_accuracy: 0.8361\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3865 - accuracy: 0.8898 - val_loss: 0.7371 - val_accuracy: 0.8389\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3824 - accuracy: 0.8861 - val_loss: 0.7422 - val_accuracy: 0.8389\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3734 - accuracy: 0.8917 - val_loss: 0.7002 - val_accuracy: 0.8431\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3705 - accuracy: 0.8944 - val_loss: 0.7647 - val_accuracy: 0.8375\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3917 - accuracy: 0.8852 - val_loss: 0.7153 - val_accuracy: 0.8389\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3931 - accuracy: 0.8884 - val_loss: 0.7488 - val_accuracy: 0.8389\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3705 - accuracy: 0.8949 - val_loss: 0.8004 - val_accuracy: 0.8375\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3982 - accuracy: 0.8875 - val_loss: 0.7324 - val_accuracy: 0.8361\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3792 - accuracy: 0.8972 - val_loss: 0.7147 - val_accuracy: 0.8417\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3664 - accuracy: 0.8935 - val_loss: 0.7140 - val_accuracy: 0.8472\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3819 - accuracy: 0.8935 - val_loss: 0.7203 - val_accuracy: 0.8389\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3764 - accuracy: 0.8968 - val_loss: 0.7532 - val_accuracy: 0.8375\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3623 - accuracy: 0.8949 - val_loss: 0.7782 - val_accuracy: 0.8417\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3617 - accuracy: 0.8991 - val_loss: 0.7389 - val_accuracy: 0.8431\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3662 - accuracy: 0.8907 - val_loss: 0.8004 - val_accuracy: 0.8361\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3756 - accuracy: 0.8935 - val_loss: 0.7765 - val_accuracy: 0.8375\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3562 - accuracy: 0.8954 - val_loss: 0.7431 - val_accuracy: 0.8444\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3785 - accuracy: 0.8912 - val_loss: 0.7551 - val_accuracy: 0.8403\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3578 - accuracy: 0.8981 - val_loss: 0.6585 - val_accuracy: 0.8458\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3628 - accuracy: 0.8977 - val_loss: 0.7606 - val_accuracy: 0.8389\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3669 - accuracy: 0.8963 - val_loss: 0.7547 - val_accuracy: 0.8431\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3721 - accuracy: 0.8921 - val_loss: 0.7461 - val_accuracy: 0.8361\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3538 - accuracy: 0.8917 - val_loss: 0.7299 - val_accuracy: 0.8458\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3796 - accuracy: 0.8931 - val_loss: 0.7260 - val_accuracy: 0.8431\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3696 - accuracy: 0.9023 - val_loss: 0.7659 - val_accuracy: 0.8403\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3646 - accuracy: 0.8912 - val_loss: 0.7252 - val_accuracy: 0.8514\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3692 - accuracy: 0.8963 - val_loss: 0.7264 - val_accuracy: 0.8389\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3677 - accuracy: 0.9000 - val_loss: 0.7708 - val_accuracy: 0.8375\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3568 - accuracy: 0.8991 - val_loss: 0.7545 - val_accuracy: 0.8403\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3568 - accuracy: 0.9019 - val_loss: 0.7340 - val_accuracy: 0.8403\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3438 - accuracy: 0.9042 - val_loss: 0.7250 - val_accuracy: 0.8417\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3540 - accuracy: 0.9051 - val_loss: 0.7388 - val_accuracy: 0.8417\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3655 - accuracy: 0.9005 - val_loss: 0.7368 - val_accuracy: 0.8431\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3557 - accuracy: 0.8968 - val_loss: 0.7303 - val_accuracy: 0.8431\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.4021 - accuracy: 0.8926 - val_loss: 0.7477 - val_accuracy: 0.8444\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3694 - accuracy: 0.8926 - val_loss: 0.7834 - val_accuracy: 0.8375\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3581 - accuracy: 0.8981 - val_loss: 0.7208 - val_accuracy: 0.8458\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3596 - accuracy: 0.9009 - val_loss: 0.7711 - val_accuracy: 0.8361\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3534 - accuracy: 0.9009 - val_loss: 0.7593 - val_accuracy: 0.8403\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3520 - accuracy: 0.9019 - val_loss: 0.7572 - val_accuracy: 0.8444\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3555 - accuracy: 0.9005 - val_loss: 0.7298 - val_accuracy: 0.8431\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3461 - accuracy: 0.9019 - val_loss: 0.7642 - val_accuracy: 0.8417\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3712 - accuracy: 0.8931 - val_loss: 0.7645 - val_accuracy: 0.8417\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3467 - accuracy: 0.9060 - val_loss: 0.7792 - val_accuracy: 0.8375\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3671 - accuracy: 0.8921 - val_loss: 0.7041 - val_accuracy: 0.8528\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3735 - accuracy: 0.8972 - val_loss: 0.7098 - val_accuracy: 0.8444\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3683 - accuracy: 0.8991 - val_loss: 0.7739 - val_accuracy: 0.8389\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3395 - accuracy: 0.8954 - val_loss: 0.7518 - val_accuracy: 0.8417\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3420 - accuracy: 0.9028 - val_loss: 0.7178 - val_accuracy: 0.8444\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3362 - accuracy: 0.9046 - val_loss: 0.8046 - val_accuracy: 0.8444\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3546 - accuracy: 0.9005 - val_loss: 0.7318 - val_accuracy: 0.8486\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3514 - accuracy: 0.9060 - val_loss: 0.7233 - val_accuracy: 0.8458\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3469 - accuracy: 0.9023 - val_loss: 0.7811 - val_accuracy: 0.8389\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3476 - accuracy: 0.9009 - val_loss: 0.7351 - val_accuracy: 0.8486\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3557 - accuracy: 0.9014 - val_loss: 0.8397 - val_accuracy: 0.8375\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3347 - accuracy: 0.9042 - val_loss: 0.7666 - val_accuracy: 0.8417\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3544 - accuracy: 0.8986 - val_loss: 0.8040 - val_accuracy: 0.8403\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3501 - accuracy: 0.8926 - val_loss: 0.7486 - val_accuracy: 0.8472\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3379 - accuracy: 0.9037 - val_loss: 0.8324 - val_accuracy: 0.8389\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3579 - accuracy: 0.8949 - val_loss: 0.7529 - val_accuracy: 0.8458\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3266 - accuracy: 0.9111 - val_loss: 0.7317 - val_accuracy: 0.8389\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3340 - accuracy: 0.9037 - val_loss: 0.7095 - val_accuracy: 0.8500\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3297 - accuracy: 0.9125 - val_loss: 0.7968 - val_accuracy: 0.8417\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3372 - accuracy: 0.9005 - val_loss: 0.7921 - val_accuracy: 0.8444\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3579 - accuracy: 0.9014 - val_loss: 0.7839 - val_accuracy: 0.8375\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3640 - accuracy: 0.8977 - val_loss: 0.6994 - val_accuracy: 0.8417\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3378 - accuracy: 0.9042 - val_loss: 0.7772 - val_accuracy: 0.8431\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3575 - accuracy: 0.8991 - val_loss: 0.7735 - val_accuracy: 0.8514\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3224 - accuracy: 0.9079 - val_loss: 0.7965 - val_accuracy: 0.8389\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3459 - accuracy: 0.9014 - val_loss: 0.7964 - val_accuracy: 0.8403\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3511 - accuracy: 0.8977 - val_loss: 0.8077 - val_accuracy: 0.8403\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3428 - accuracy: 0.9037 - val_loss: 0.7884 - val_accuracy: 0.8417\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3416 - accuracy: 0.9028 - val_loss: 0.7682 - val_accuracy: 0.8458\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3525 - accuracy: 0.8944 - val_loss: 0.7217 - val_accuracy: 0.8417\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3481 - accuracy: 0.9069 - val_loss: 0.7701 - val_accuracy: 0.8431\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3301 - accuracy: 0.9069 - val_loss: 0.7766 - val_accuracy: 0.8500\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3128 - accuracy: 0.9097 - val_loss: 0.7684 - val_accuracy: 0.8486\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3523 - accuracy: 0.9014 - val_loss: 0.7679 - val_accuracy: 0.8417\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3346 - accuracy: 0.9019 - val_loss: 0.7477 - val_accuracy: 0.8431\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3386 - accuracy: 0.9032 - val_loss: 0.7923 - val_accuracy: 0.8444\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3325 - accuracy: 0.9051 - val_loss: 0.7598 - val_accuracy: 0.8500\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3295 - accuracy: 0.9051 - val_loss: 0.7888 - val_accuracy: 0.8431\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3331 - accuracy: 0.9037 - val_loss: 0.8051 - val_accuracy: 0.8472\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3410 - accuracy: 0.9019 - val_loss: 0.7787 - val_accuracy: 0.8417\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3364 - accuracy: 0.9069 - val_loss: 0.7938 - val_accuracy: 0.8403\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3339 - accuracy: 0.9005 - val_loss: 0.8077 - val_accuracy: 0.8375\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3400 - accuracy: 0.9005 - val_loss: 0.8080 - val_accuracy: 0.8431\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3327 - accuracy: 0.9079 - val_loss: 0.6979 - val_accuracy: 0.8514\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3387 - accuracy: 0.9148 - val_loss: 0.8104 - val_accuracy: 0.8431\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3314 - accuracy: 0.9120 - val_loss: 0.7952 - val_accuracy: 0.8472\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3223 - accuracy: 0.9046 - val_loss: 0.8185 - val_accuracy: 0.8458\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3488 - accuracy: 0.8972 - val_loss: 0.7546 - val_accuracy: 0.8417\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3220 - accuracy: 0.9051 - val_loss: 0.7337 - val_accuracy: 0.8542\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3075 - accuracy: 0.9060 - val_loss: 0.7375 - val_accuracy: 0.8472\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3359 - accuracy: 0.9102 - val_loss: 0.7877 - val_accuracy: 0.8486\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3498 - accuracy: 0.9037 - val_loss: 0.7593 - val_accuracy: 0.8444\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3317 - accuracy: 0.9023 - val_loss: 0.7670 - val_accuracy: 0.8500\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.62779\n",
      "2160/2160 - 239s - loss: 0.3346 - accuracy: 0.9069 - val_loss: 0.7571 - val_accuracy: 0.8472\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3426 - accuracy: 0.9056 - val_loss: 0.8172 - val_accuracy: 0.8458\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3373 - accuracy: 0.9032 - val_loss: 0.7697 - val_accuracy: 0.8444\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.62779\n",
      "2160/2160 - 7s - loss: 0.3354 - accuracy: 0.9009 - val_loss: 0.8164 - val_accuracy: 0.8556\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.62779\n",
      "2160/2160 - 105s - loss: 0.3241 - accuracy: 0.9120 - val_loss: 0.8337 - val_accuracy: 0.8472\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3297 - accuracy: 0.9125 - val_loss: 0.8133 - val_accuracy: 0.8500\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.62779\n",
      "2160/2160 - 18s - loss: 0.3319 - accuracy: 0.9032 - val_loss: 0.8104 - val_accuracy: 0.8486\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.62779\n",
      "2160/2160 - 43s - loss: 0.3148 - accuracy: 0.9111 - val_loss: 0.7988 - val_accuracy: 0.8472\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3492 - accuracy: 0.9032 - val_loss: 0.7992 - val_accuracy: 0.8458\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3394 - accuracy: 0.9065 - val_loss: 0.7911 - val_accuracy: 0.8431\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3280 - accuracy: 0.9056 - val_loss: 0.7374 - val_accuracy: 0.8458\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.62779\n",
      "2160/2160 - 28s - loss: 0.3339 - accuracy: 0.9074 - val_loss: 0.7548 - val_accuracy: 0.8486\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3145 - accuracy: 0.9111 - val_loss: 0.7525 - val_accuracy: 0.8458\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3166 - accuracy: 0.9060 - val_loss: 0.8092 - val_accuracy: 0.8403\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3371 - accuracy: 0.9083 - val_loss: 0.8222 - val_accuracy: 0.8528\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3205 - accuracy: 0.9134 - val_loss: 0.8453 - val_accuracy: 0.8472\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3228 - accuracy: 0.9097 - val_loss: 0.7862 - val_accuracy: 0.8528\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3232 - accuracy: 0.9134 - val_loss: 0.7980 - val_accuracy: 0.8500\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3178 - accuracy: 0.9125 - val_loss: 0.8142 - val_accuracy: 0.8486\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3528 - accuracy: 0.8991 - val_loss: 0.7670 - val_accuracy: 0.8444\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3199 - accuracy: 0.9074 - val_loss: 0.7827 - val_accuracy: 0.8528\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3261 - accuracy: 0.9097 - val_loss: 0.7562 - val_accuracy: 0.8458\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3224 - accuracy: 0.9093 - val_loss: 0.7424 - val_accuracy: 0.8569\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3317 - accuracy: 0.9134 - val_loss: 0.8350 - val_accuracy: 0.8514\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3173 - accuracy: 0.9130 - val_loss: 0.7877 - val_accuracy: 0.8472\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3260 - accuracy: 0.9097 - val_loss: 0.8010 - val_accuracy: 0.8458\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3342 - accuracy: 0.9037 - val_loss: 0.7660 - val_accuracy: 0.8431\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3231 - accuracy: 0.9069 - val_loss: 0.7268 - val_accuracy: 0.8500\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3308 - accuracy: 0.9069 - val_loss: 0.8321 - val_accuracy: 0.8444\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3307 - accuracy: 0.9102 - val_loss: 0.7942 - val_accuracy: 0.8458\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3084 - accuracy: 0.9162 - val_loss: 0.8575 - val_accuracy: 0.8431\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3045 - accuracy: 0.9069 - val_loss: 0.8002 - val_accuracy: 0.8472\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3320 - accuracy: 0.9065 - val_loss: 0.7299 - val_accuracy: 0.8542\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3262 - accuracy: 0.9134 - val_loss: 0.7728 - val_accuracy: 0.8472\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3141 - accuracy: 0.9083 - val_loss: 0.7887 - val_accuracy: 0.8472\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3345 - accuracy: 0.9079 - val_loss: 0.7993 - val_accuracy: 0.8444\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3280 - accuracy: 0.9125 - val_loss: 0.7167 - val_accuracy: 0.8500\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3207 - accuracy: 0.9111 - val_loss: 0.7123 - val_accuracy: 0.8472\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3243 - accuracy: 0.9093 - val_loss: 0.7984 - val_accuracy: 0.8528\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3441 - accuracy: 0.9005 - val_loss: 0.8360 - val_accuracy: 0.8431\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3048 - accuracy: 0.9190 - val_loss: 0.8358 - val_accuracy: 0.8500\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3272 - accuracy: 0.9093 - val_loss: 0.8277 - val_accuracy: 0.8458\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3275 - accuracy: 0.9083 - val_loss: 0.8313 - val_accuracy: 0.8458\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3332 - accuracy: 0.9046 - val_loss: 0.7352 - val_accuracy: 0.8458\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3142 - accuracy: 0.9083 - val_loss: 0.8647 - val_accuracy: 0.8417\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3237 - accuracy: 0.9074 - val_loss: 0.7490 - val_accuracy: 0.8528\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3200 - accuracy: 0.9088 - val_loss: 0.7740 - val_accuracy: 0.8514\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3126 - accuracy: 0.9083 - val_loss: 0.8013 - val_accuracy: 0.8417\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3235 - accuracy: 0.9134 - val_loss: 0.7583 - val_accuracy: 0.8514\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3199 - accuracy: 0.9106 - val_loss: 0.7782 - val_accuracy: 0.8556\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3420 - accuracy: 0.9037 - val_loss: 0.6999 - val_accuracy: 0.8458\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3234 - accuracy: 0.9079 - val_loss: 0.7744 - val_accuracy: 0.8500\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3141 - accuracy: 0.9083 - val_loss: 0.7755 - val_accuracy: 0.8528\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3286 - accuracy: 0.9028 - val_loss: 0.7863 - val_accuracy: 0.8542\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3155 - accuracy: 0.9153 - val_loss: 0.8046 - val_accuracy: 0.8514\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3347 - accuracy: 0.9065 - val_loss: 0.7927 - val_accuracy: 0.8500\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3306 - accuracy: 0.9065 - val_loss: 0.7326 - val_accuracy: 0.8514\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3098 - accuracy: 0.9134 - val_loss: 0.7903 - val_accuracy: 0.8458\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3117 - accuracy: 0.9185 - val_loss: 0.7757 - val_accuracy: 0.8458\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3387 - accuracy: 0.9051 - val_loss: 0.7339 - val_accuracy: 0.8472\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3102 - accuracy: 0.9120 - val_loss: 0.8625 - val_accuracy: 0.8486\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3330 - accuracy: 0.9014 - val_loss: 0.7770 - val_accuracy: 0.8486\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3379 - accuracy: 0.9088 - val_loss: 0.7940 - val_accuracy: 0.8542\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3297 - accuracy: 0.9088 - val_loss: 0.7795 - val_accuracy: 0.8486\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3218 - accuracy: 0.9102 - val_loss: 0.8721 - val_accuracy: 0.8389\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3221 - accuracy: 0.9111 - val_loss: 0.7212 - val_accuracy: 0.8514\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3019 - accuracy: 0.9093 - val_loss: 0.7654 - val_accuracy: 0.8514\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3120 - accuracy: 0.9153 - val_loss: 0.7958 - val_accuracy: 0.8500\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3120 - accuracy: 0.9056 - val_loss: 0.7933 - val_accuracy: 0.8556\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3178 - accuracy: 0.9130 - val_loss: 0.8331 - val_accuracy: 0.8458\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3103 - accuracy: 0.9074 - val_loss: 0.8602 - val_accuracy: 0.8472\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3133 - accuracy: 0.9088 - val_loss: 0.8480 - val_accuracy: 0.8458\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3003 - accuracy: 0.9083 - val_loss: 0.7692 - val_accuracy: 0.8472\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3344 - accuracy: 0.9060 - val_loss: 0.7785 - val_accuracy: 0.8486\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3254 - accuracy: 0.9106 - val_loss: 0.8062 - val_accuracy: 0.8528\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3169 - accuracy: 0.9148 - val_loss: 0.8115 - val_accuracy: 0.8472\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3103 - accuracy: 0.9144 - val_loss: 0.8066 - val_accuracy: 0.8458\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3395 - accuracy: 0.9079 - val_loss: 0.8533 - val_accuracy: 0.8403\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3168 - accuracy: 0.9148 - val_loss: 0.8224 - val_accuracy: 0.8500\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2949 - accuracy: 0.9125 - val_loss: 0.8648 - val_accuracy: 0.8500\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3289 - accuracy: 0.9111 - val_loss: 0.8508 - val_accuracy: 0.8514\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3290 - accuracy: 0.9060 - val_loss: 0.7057 - val_accuracy: 0.8500\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3437 - accuracy: 0.9000 - val_loss: 0.7454 - val_accuracy: 0.8458\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3002 - accuracy: 0.9102 - val_loss: 0.8022 - val_accuracy: 0.8472\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3181 - accuracy: 0.9042 - val_loss: 0.8237 - val_accuracy: 0.8486\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3033 - accuracy: 0.9194 - val_loss: 0.8415 - val_accuracy: 0.8500\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3366 - accuracy: 0.9028 - val_loss: 0.8210 - val_accuracy: 0.8458\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3044 - accuracy: 0.9134 - val_loss: 0.8036 - val_accuracy: 0.8472\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3052 - accuracy: 0.9157 - val_loss: 0.8059 - val_accuracy: 0.8486\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3054 - accuracy: 0.9120 - val_loss: 0.7881 - val_accuracy: 0.8514\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3040 - accuracy: 0.9148 - val_loss: 0.7927 - val_accuracy: 0.8486\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3132 - accuracy: 0.9111 - val_loss: 0.8243 - val_accuracy: 0.8514\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2986 - accuracy: 0.9176 - val_loss: 0.8421 - val_accuracy: 0.8472\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3420 - accuracy: 0.8995 - val_loss: 0.8399 - val_accuracy: 0.8500\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3040 - accuracy: 0.9157 - val_loss: 0.8961 - val_accuracy: 0.8458\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3240 - accuracy: 0.9074 - val_loss: 0.7292 - val_accuracy: 0.8500\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3215 - accuracy: 0.9139 - val_loss: 0.7606 - val_accuracy: 0.8500\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3184 - accuracy: 0.9088 - val_loss: 0.8293 - val_accuracy: 0.8514\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2968 - accuracy: 0.9148 - val_loss: 0.8295 - val_accuracy: 0.8472\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3126 - accuracy: 0.9130 - val_loss: 0.7400 - val_accuracy: 0.8486\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3211 - accuracy: 0.9125 - val_loss: 0.7399 - val_accuracy: 0.8472\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3128 - accuracy: 0.9130 - val_loss: 0.7809 - val_accuracy: 0.8542\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3187 - accuracy: 0.9093 - val_loss: 0.8607 - val_accuracy: 0.8486\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3248 - accuracy: 0.9097 - val_loss: 0.8231 - val_accuracy: 0.8458\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3098 - accuracy: 0.9153 - val_loss: 0.7242 - val_accuracy: 0.8528\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3199 - accuracy: 0.9097 - val_loss: 0.7658 - val_accuracy: 0.8556\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3236 - accuracy: 0.9111 - val_loss: 0.7859 - val_accuracy: 0.8500\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3123 - accuracy: 0.9139 - val_loss: 0.9040 - val_accuracy: 0.8472\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2991 - accuracy: 0.9194 - val_loss: 0.9107 - val_accuracy: 0.8431\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3120 - accuracy: 0.9120 - val_loss: 0.8231 - val_accuracy: 0.8500\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3191 - accuracy: 0.9102 - val_loss: 0.7275 - val_accuracy: 0.8514\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3012 - accuracy: 0.9204 - val_loss: 0.8104 - val_accuracy: 0.8486\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3076 - accuracy: 0.9181 - val_loss: 0.8898 - val_accuracy: 0.8500\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3034 - accuracy: 0.9167 - val_loss: 0.7938 - val_accuracy: 0.8528\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3108 - accuracy: 0.9130 - val_loss: 0.7795 - val_accuracy: 0.8500\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3017 - accuracy: 0.9157 - val_loss: 0.8577 - val_accuracy: 0.8417\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3306 - accuracy: 0.9051 - val_loss: 0.7907 - val_accuracy: 0.8472\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3048 - accuracy: 0.9171 - val_loss: 0.7929 - val_accuracy: 0.8514\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3346 - accuracy: 0.9088 - val_loss: 0.7740 - val_accuracy: 0.8472\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2910 - accuracy: 0.9204 - val_loss: 0.8058 - val_accuracy: 0.8486\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3047 - accuracy: 0.9176 - val_loss: 0.8130 - val_accuracy: 0.8528\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2929 - accuracy: 0.9144 - val_loss: 0.8195 - val_accuracy: 0.8542\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3171 - accuracy: 0.9093 - val_loss: 0.7478 - val_accuracy: 0.8528\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3245 - accuracy: 0.9060 - val_loss: 0.7603 - val_accuracy: 0.8500\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3124 - accuracy: 0.9139 - val_loss: 0.8607 - val_accuracy: 0.8514\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2961 - accuracy: 0.9125 - val_loss: 0.8045 - val_accuracy: 0.8486\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.62779\n",
      "2160/2160 - 6s - loss: 0.3056 - accuracy: 0.9106 - val_loss: 0.7891 - val_accuracy: 0.8528\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3182 - accuracy: 0.9088 - val_loss: 0.8715 - val_accuracy: 0.8472\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2960 - accuracy: 0.9218 - val_loss: 0.7985 - val_accuracy: 0.8528\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2976 - accuracy: 0.9162 - val_loss: 0.8669 - val_accuracy: 0.8500\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3116 - accuracy: 0.9153 - val_loss: 0.8591 - val_accuracy: 0.8500\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3095 - accuracy: 0.9148 - val_loss: 0.8173 - val_accuracy: 0.8556\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.3192 - accuracy: 0.9157 - val_loss: 0.8328 - val_accuracy: 0.8514\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.62779\n",
      "2160/2160 - 5s - loss: 0.2957 - accuracy: 0.9176 - val_loss: 0.7883 - val_accuracy: 0.8542\n"
     ]
    }
   ],
   "source": [
    "#EEGNet Training\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from EEGModels import EEGNet\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# PyRiemann imports\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# tools for plotting confusion matrices\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# while the default tensorflow ordering is 'channels_last' we set it here\n",
    "# to be explicit in case if the user has changed the default ordering\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "# extract raw data. scale by 1000 due to scaling sensitivity in deep learning\n",
    "X = Train_data # format is in (trials, channels, samples)\n",
    "y = Train_labels\n",
    "kernels, chans, samples = 1, 32, 192\n",
    "\n",
    "\n",
    "# take 60/20/20 percent of the data to train/validate/test\n",
    "End_index = len(Train_labels)\n",
    "#End_index = len(Ys_train)\n",
    "Train_index = (int) (End_index * 0.6)\n",
    "Val_index = (int) (End_index * 0.8)\n",
    "\n",
    "X_train      = X[0:Train_index,]\n",
    "Y_train      = y[0:Train_index]\n",
    "X_validate   = X[Train_index:Val_index,]\n",
    "Y_validate   = y[Train_index:Val_index]\n",
    "X_test       = X[Val_index:,]\n",
    "Y_test       = y[Val_index:]\n",
    "\n",
    "\n",
    "# Split daa for RG+xDrawn training done in later steps\n",
    "RG_train_data      = X[0:Val_index,]\n",
    "RG_train_labels      = y[0:Val_index]\n",
    "RG_test_data       = X[Val_index:,]\n",
    "RG_test_labels       = y[Val_index:] \n",
    "\n",
    "\n",
    "\n",
    "#print(Y_train)\n",
    "############################# EEGNet portion ##################################\n",
    "\n",
    "# convert labels to one-hot encodings.\n",
    "Y_train      = np_utils.to_categorical(Y_train,  num_classes=2)\n",
    "Y_validate   = np_utils.to_categorical(Y_validate,  num_classes=2)\n",
    "Y_test       = np_utils.to_categorical(Y_test,  num_classes=2)\n",
    "RG_train_labels      = np_utils.to_categorical(RG_train_labels,  num_classes=2)\n",
    "RG_test_labels       = np_utils.to_categorical(RG_test_labels,  num_classes=2)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format. Data \n",
    "# contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
    "X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "X_validate   = X_validate.reshape(X_validate.shape[0], chans, samples, kernels)\n",
    "X_test       = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "print(np.shape(X_train))   \n",
    "\n",
    "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
    "# model configurations may do better, but this is a good starting point)\n",
    "model = EEGNet(nb_classes = 2, Chans = chans, Samples = samples, \n",
    "               dropoutRate = 0.5, kernLength = 32, F1 = 8, D = 2, F2 = 16, \n",
    "               dropoutType = 'Dropout')\n",
    "\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# count number of parameters in the model\n",
    "numParams    = model.count_params()    \n",
    "\n",
    "# set a valid path for your system to record model checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "###############################################################################\n",
    "# if the classification task was imbalanced (significantly more trials in one\n",
    "# class versus the others) you can assign a weight to each class during \n",
    "# optimization to balance it out. This data is approximately balanced so we \n",
    "# don't need to do this, but is shown here for illustration/completeness. \n",
    "###############################################################################\n",
    "\n",
    "# the syntax is {class_1:weight_1, class_2:weight_2,...}. Here just setting\n",
    "# the weights all to be 1\n",
    "class_weights = {0:5, 1:1}\n",
    "\n",
    "################################################################################\n",
    "# fit the model. Due to very small sample sizes this can get\n",
    "# pretty noisy run-to-run, but most runs should be comparable to xDAWN + \n",
    "# Riemannian geometry classification (below)\n",
    "################################################################################\n",
    "fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 300, \n",
    "                        verbose = 2, validation_data=(X_validate, Y_validate),\n",
    "                        callbacks=[checkpointer], class_weight = class_weights)\n",
    "\n",
    "# load optimal weights\n",
    "model.load_weights('/tmp/checkpoint.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ca5898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2304 samples, validate on 576 samples\n",
      "Epoch 1/500\n",
      "2304/2304 [==============================] - 30s 13ms/sample - loss: 0.5158 - accuracy: 0.7704 - val_loss: 0.4279 - val_accuracy: 0.8385\n",
      "Epoch 2/500\n",
      "2304/2304 [==============================] - 26s 12ms/sample - loss: 0.3915 - accuracy: 0.8390 - val_loss: 0.3819 - val_accuracy: 0.8385\n",
      "Epoch 3/500\n",
      "2304/2304 [==============================] - 27s 12ms/sample - loss: 0.3417 - accuracy: 0.8628 - val_loss: 0.3701 - val_accuracy: 0.8438\n",
      "Epoch 4/500\n",
      "2304/2304 [==============================] - 26s 11ms/sample - loss: 0.3179 - accuracy: 0.8759 - val_loss: 0.3703 - val_accuracy: 0.8420\n",
      "Epoch 5/500\n",
      "2304/2304 [==============================] - 26s 11ms/sample - loss: 0.3020 - accuracy: 0.8811 - val_loss: 0.3611 - val_accuracy: 0.8472\n",
      "Epoch 6/500\n",
      "2304/2304 [==============================] - 27s 12ms/sample - loss: 0.2844 - accuracy: 0.8889 - val_loss: 0.3675 - val_accuracy: 0.8472\n",
      "Epoch 7/500\n",
      "2304/2304 [==============================] - 26s 11ms/sample - loss: 0.2488 - accuracy: 0.9076 - val_loss: 0.4133 - val_accuracy: 0.8385\n",
      "Epoch 8/500\n",
      "2304/2304 [==============================] - 27s 12ms/sample - loss: 0.2709 - accuracy: 0.9006 - val_loss: 0.3616 - val_accuracy: 0.8472\n",
      "Epoch 9/500\n",
      "2304/2304 [==============================] - 28s 12ms/sample - loss: 0.2351 - accuracy: 0.9132 - val_loss: 0.3970 - val_accuracy: 0.8420\n",
      "Epoch 10/500\n",
      "2304/2304 [==============================] - 27s 12ms/sample - loss: 0.2349 - accuracy: 0.9119 - val_loss: 0.3799 - val_accuracy: 0.8490\n",
      "Epoch 11/500\n",
      "2304/2304 [==============================] - 28s 12ms/sample - loss: 0.2271 - accuracy: 0.9188 - val_loss: 0.4239 - val_accuracy: 0.8438\n",
      "Epoch 12/500\n",
      "2304/2304 [==============================] - 28s 12ms/sample - loss: 0.2205 - accuracy: 0.9188 - val_loss: 0.4476 - val_accuracy: 0.8472\n",
      "Epoch 13/500\n",
      "2304/2304 [==============================] - 28s 12ms/sample - loss: 0.2081 - accuracy: 0.9232 - val_loss: 0.4393 - val_accuracy: 0.8490\n",
      "Epoch 14/500\n",
      "2304/2304 [==============================] - 28s 12ms/sample - loss: 0.2106 - accuracy: 0.9258 - val_loss: 0.4269 - val_accuracy: 0.8507\n",
      "Epoch 15/500\n",
      "2288/2304 [============================>.] - ETA: 0s - loss: 0.2032 - accuracy: 0.9270Restoring model weights from the end of the best epoch.\n",
      "2304/2304 [==============================] - 27s 12ms/sample - loss: 0.2028 - accuracy: 0.9271 - val_loss: 0.4715 - val_accuracy: 0.8455\n",
      "Epoch 00015: early stopping\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "#EEG Inception Training\n",
    "\n",
    "\n",
    "#%% IMPORT LIBRARIES\n",
    "from EEG-Inception import EEGInception\n",
    "import numpy as np\n",
    "import h5py, os\n",
    "#from EEGInception import EEGInception\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#%% PARAMETERS\n",
    "\n",
    "#dataset_path = 'Desktop/GIB-UVA ERP-BCI.hdf5'\n",
    "\n",
    "#%% HYPERPARAMETERS\n",
    "\n",
    "input_time = 1000\n",
    "fs = 192\n",
    "n_cha = 32\n",
    "filters_per_branch = 8\n",
    "scales_time = (500, 250, 125)\n",
    "dropout_rate = 0.25\n",
    "activation = 'elu'\n",
    "n_classes = 2\n",
    "learning_rate = 0.001\n",
    "\n",
    "#features, features_test, erp_labels, labels_test = train_test_split(Train_data_inception, Train_labels, test_size=0.20, random_state=42)\n",
    "\n",
    "End_index = len(Train_labels)\n",
    "Train_index = (int) (End_index * 0.8)\n",
    "\n",
    "features=Train_data_inception[0:Train_index,]\n",
    "erp_labels = Train_labels[0:Train_index,]\n",
    "features_test = Train_data_inception[Train_index:,]\n",
    "labels_test = Train_labels[Train_index:]\n",
    "                          \n",
    "\n",
    "#%% PREPARE FEATURES AND LABELS\n",
    "# Reshape epochs for EEG-Inception\n",
    "features_test = features_test.reshape(\n",
    "    (features_test.shape[0], features_test.shape[1],\n",
    "     features_test.shape[2], 1)\n",
    ")\n",
    "features = features.reshape(\n",
    "    (features.shape[0], features.shape[1],\n",
    "     features.shape[2], 1)\n",
    ")\n",
    "\n",
    "# One hot encoding of labels\n",
    "def one_hot_labels(caategorical_labels):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    on_hot_labels = enc.fit_transform(\n",
    "        caategorical_labels.reshape(-1, 1)).toarray()\n",
    "    return on_hot_labels\n",
    "\n",
    "\n",
    "train_erp_labels = one_hot_labels(erp_labels)\n",
    "labels_test = one_hot_labels(labels_test)    \n",
    "\n",
    "# Create model\n",
    "inception_model = EEGInception(\n",
    "   input_time=1000, fs=192, ncha=32, filters_per_branch=8,\n",
    "   scales_time=(500, 250, 125), dropout_rate=0.25,\n",
    "   activation='elu', n_classes=2, learning_rate=0.001)\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0.0001,\n",
    "    mode='min', patience=10, verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Fit model\n",
    "fit_hist = inception_model.fit(features,\n",
    "                     train_erp_labels,\n",
    "                     epochs=500,\n",
    "                     batch_size=16,\n",
    "                     validation_split=0.2,\n",
    "                     callbacks=[early_stopping])\n",
    "\n",
    "# Save\n",
    "inception_model.save('model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42442c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65112434],\n",
       "       [-2.98146503],\n",
       "       [-0.3456152 ],\n",
       "       ...,\n",
       "       [-0.19343383],\n",
       "       [-0.29470154],\n",
       "       [ 0.12836426]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rLDA Model Training\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "\n",
    "\n",
    "\n",
    "X_lda = Train_data_flattened # format is in (trials, channels, samples)\n",
    "y_lda = Train_labels\n",
    "kernels, chans, samples = 1, 32, 128\n",
    "\n",
    "End_index_lda = len(Train_labels)\n",
    "Train_index_lda = (int) (End_index_lda * 0.8)\n",
    "X_lda_train      = X_lda[0:Train_index_lda,]\n",
    "Y_lda_train      = y_lda[0:Train_index_lda]\n",
    "X_lda_test       = X_lda[Train_index_lda:,]\n",
    "Y_lda_test       = y_lda[Train_index_lda:]\n",
    "\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\n",
    "lda_model.fit_transform(X_lda_train, Y_lda_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c5eaf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('xdawncovariances', XdawnCovariances(nfilter=2)),\n",
       "                ('tangentspace', TangentSpace()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyRiemann Training \n",
    "\n",
    "# code is taken from PyRiemann's ERP sample script, which is decoding in \n",
    "# the tangent space with a logistic regression\n",
    "\n",
    "#End_index_rg = len(Train_labels)\n",
    "#Train_index_rg = (int) (End_index_rg * 0.8)\n",
    "#RG_train      = X[0:Train_index_rg,]\n",
    "#RG_train      = y[0:Train_index_rg]\n",
    "#RG_test       = X[Train_index_rg:,]\n",
    "#RG_test       = y[Train_index_rg:]\n",
    "#print(np.shape(RG_train))\n",
    "n_components = 2  # pick some components\n",
    "\n",
    "# set up sklearn pipeline\n",
    "clf = make_pipeline(XdawnCovariances(n_components),\n",
    "                    TangentSpace(metric='riemann'),\n",
    "                    LogisticRegression())\n",
    "\n",
    "preds_rg     = np.zeros(len(RG_test_data))\n",
    "#print(np.shape(X_train))\n",
    "# reshape back to (trials, channels, samples)\n",
    "#RG_train      = RG_train.reshape(RG_train.shape[0], chans, )\n",
    "#RG_test       = RG_test.reshape(RG_test.shape[0], chans, samples)\n",
    "\n",
    "# train a classifier with xDAWN spatial filtering + Riemannian Geometry (RG)\n",
    "# labels need to be back in single-column format\n",
    "clf.fit(RG_train_data, RG_train_labels.argmax(axis = -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7be32e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGNet-8.2 Classification accuracy: 0.833333 \n",
      "EEG Inception Classification accuracy: 0.833333 \n",
      "xDAWN + RG Classification accuracy: 0.856944 \n",
      "LDA Classification accuracy: 0.830556 \n",
      "EEGNet Overall Categorical Accuracy: 83.33%\n",
      "              Precision  Recall   F-Score  Support\n",
      "Non-Attended   0.833333     1.0  0.909091    600.0\n",
      "Attended       0.000000     0.0  0.000000    120.0\n",
      "EEGInception Overall Categorical Accuracy: 83.33%\n",
      "              Precision  Recall   F-Score  Support\n",
      "Non-Attended   0.833333     1.0  0.909091    600.0\n",
      "Attended       0.000000     0.0  0.000000    120.0\n",
      "RG+xDawn Overall Categorical Accuracy: 85.69%\n",
      "              Precision    Recall   F-Score  Support\n",
      "Non-Attended   0.869242  0.975000  0.919089    600.0\n",
      "Attended       0.680851  0.266667  0.383234    120.0\n",
      "LDA Overall Categorical Accuracy: 83.06%\n",
      "              Precision    Recall   F-Score  Support\n",
      "Non-Attended   0.833799  0.995000  0.907295    600.0\n",
      "Attended       0.250000  0.008333  0.016129    120.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashwanthravipati/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yashwanthravipati/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Test the models and outout confusion matrix, precision and recall\n",
    "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.set(font_scale=1.0)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        square=True, \n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        fmt='d', \n",
    "        cmap=plt.cm.Blues,\n",
    "        cbar=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(title + \": Confusion Matrix\", fontsize=12)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha=\"right\")\n",
    "    ax.set_ylabel('Predicted Label', fontsize=12)\n",
    "    ax.set_xlabel('True Label', fontsize=12)\n",
    "\n",
    "def display_results(y_true, y_preds, class_labels,method):\n",
    "    \n",
    "    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds),\n",
    "                          columns=class_labels).T\n",
    "\n",
    "    results.rename(columns={0: 'Precision', 1: 'Recall',\n",
    "                            2: 'F-Score', 3: 'Support'}, inplace=True)\n",
    "    \n",
    "    results.sort_values(by='F-Score', ascending=False, inplace=True)                           \n",
    "    global_acc = accuracy_score(y_true, y_preds)\n",
    "    \n",
    "    print(method + \" Overall Categorical Accuracy: {:.2f}%\".format(global_acc*100))\n",
    "    print(results)\n",
    "    return results\n",
    "    \n",
    "names        = ['Non-Attended', 'Attended']\n",
    "\n",
    "probs       = model.predict(X_test)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == Y_test.argmax(axis=-1))\n",
    "print(\"EEGNet-8.2 Classification accuracy: %f \" % (acc))\n",
    "\n",
    "probs_i       = inception_model.predict(features_test)\n",
    "preds_i       = probs_i.argmax(axis = -1)  \n",
    "acc_i         = np.mean(preds_i == labels_test.argmax(axis=-1))\n",
    "print(\"EEG Inception Classification accuracy: %f \" % (acc_i))\n",
    "\n",
    "preds_rg     = clf.predict(RG_test_data)\n",
    "acc_r         = np.mean(preds_rg == RG_test_labels.argmax(axis = -1))\n",
    "print(\"xDAWN + RG Classification accuracy: %f \" % (acc_r))\n",
    "\n",
    "\n",
    "preds_LDA = lda_model.predict(X_lda_test)\n",
    "acc3         = np.mean(preds_LDA == Y_lda_test)\n",
    "print(\"LDA Classification accuracy: %f \" % (acc3))\n",
    "\n",
    "\n",
    " # Plot EEGNet Accuracy, Precision and recall\n",
    "\n",
    "display_results(Y_test.argmax(axis = -1), preds, names , \"EEGNet\")\n",
    "plt.figure(0)\n",
    "fig, (ax3) = plt.subplots(1, 1, figsize=(6, 5))\n",
    "plot_heatmap(preds, Y_test.argmax(axis = -1), names, ax3, title = 'EEGNet-8.2')\n",
    "\n",
    "\n",
    " # Plot EEGInception Accuracy, Precision and recall\n",
    "\n",
    "display_results(labels_test.argmax(axis = -1), preds_i, names , \"EEGInception\")\n",
    "plt.figure(0)\n",
    "fig, (ax3) = plt.subplots(1, 1, figsize=(6, 5))\n",
    "plot_heatmap(preds_i, labels_test.argmax(axis = -1), names, ax3, title = 'EEGInception')\n",
    "\n",
    "# Plot RG+xDawn Accuracy, Precision and recall\n",
    "\n",
    "display_results(RG_test_labels.argmax(axis = -1), preds_rg, names , \"RG+xDawn\")\n",
    "plt.figure(0)\n",
    "fig, (ax3) = plt.subplots(1, 1, figsize=(6, 5))\n",
    "plot_heatmap(preds_rg,RG_test_labels.argmax(axis = -1), names, ax3, title = 'RG+xDawn')\n",
    "\n",
    "\n",
    " # Plot LDA Accuracy, Precision and recall\n",
    "display_results(Y_lda_test, preds_LDA, names , \"LDA\")\n",
    "plt.figure(0)\n",
    "fig, (ax3) = plt.subplots(1, 1, figsize=(6, 5))\n",
    "plot_heatmap(preds_LDA,Y_lda_test, names, ax3, title = 'LDA')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbd4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
